{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapter Layer - all functionality that interacts with the S3\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: read_csv_to_df(bucket, key, decoding = 'utf-8', separator = ',')\n",
    "\n",
    "Purpose: Trasnform a csv file to a dataframe \n",
    "\n",
    "Explanation:\n",
    "        - Decode the csv file of the bucket\n",
    "        - Use StringIO to make it into \n",
    "            The StringIO module is an in-memory file-like object. \n",
    "            This object can be used as input or output to the most \n",
    "            function that would expect a standard file object.\n",
    "        - Create the dataframe using a pandas data frame\n",
    "            Read a comma-separated values (csv) file into DataFrame.\n",
    "Parameters: \n",
    "            Bucket, Object, Source bucket or destination bucket\n",
    "            key, String, Specific csv file that will be converted to csv\n",
    "\n",
    "Returns:\n",
    "            df, dataframe, dataframe of the csv\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def read_csv_to_df(bucket, key, decoding = 'utf-8', separator = ','):\n",
    "    csv_obj = bucket.Object(key=key).get().get('Body').read().decode(decoding)\n",
    "    data = StringIO(csv_obj)\n",
    "    df = pd.read_csv(data, delimiter=separator)\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: write_df_to_s3(bucket, df, key)\n",
    "\n",
    "Purpose: Write the df into an S3 bucket\n",
    "\n",
    "Explanation:\n",
    "        - Create an object of BytesIO.\n",
    "            This class is like StringIO for bytes objects.\n",
    "        - Trasnform the dataframe into a parquet file.\n",
    "        - Put parquet file in the S3 bucket.\n",
    "            \n",
    "What is a parquet file format?\n",
    "        Parquet is an open source file format available to any project \n",
    "        in the Hadoop ecosystem. Apache Parquet is designed for efficient \n",
    "        as well as performant flat columnar storage format of data \n",
    "        compared to row based files like CSV or TSV files. ... \n",
    "        Parquet can only read the needed columns therefore greatly \n",
    "        minimizing the IO.\n",
    "        \n",
    "Parameters: \n",
    "            Bucket, Object, Source bucket or destination bucket.\n",
    "            df, dataframe, dataframe to be changed to parquet object.\n",
    "            key, String, Specific csv file that will be converted to csv.\n",
    "\n",
    "Returns:\n",
    "            df, dataframe, dataframe of the csv\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def write_df_to_s3(bucket, df, key):\n",
    "    out_buffer = BytesIO()\n",
    "    df.to_parquet(out_buffer, index=False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: write_df_to_s3_csv(bucket, df, key)\n",
    "\n",
    "Purpose: Method to update the metafile.\n",
    "\n",
    "Explanation:\n",
    "        - Create an object of StringIO.\n",
    "        - Trasnform the dataframe into a csv file.\n",
    "        - Put csv file in the S3 bucket.\n",
    "        \n",
    "Parameters: \n",
    "            Bucket, Object, Source bucket or destination bucket.\n",
    "            df, dataframe, dataframe to be changed to parquet object.\n",
    "            key, String, Specific csv file that will be converted to csv.\n",
    "\n",
    "Returns:\n",
    "            df, dataframe, dataframe of the csv\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def write_df_to_s3_csv(bucket, df, key):\n",
    "    out_buffer = StringIO()\n",
    "    df.to_csv(out_buffer, index=False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: list_files_in_prefix(bucket, prefix)\n",
    "\n",
    "Purpose: Get the key of every object in the file\n",
    "\n",
    "Explanation:\n",
    "        - Create a list of the key of every object\n",
    "        \n",
    "        \n",
    "Parameters: \n",
    "            Bucket, Object, Source bucket or destination bucket.\n",
    "            prefix, string, date of the file\n",
    "\n",
    "Returns:\n",
    "            files, list, list of the keys \n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def list_files_in_prefix(bucket, prefix):\n",
    "    files = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    return files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application Layer\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: extract(bucket, date_list)\n",
    "\n",
    "Purpose: Extract dataframes from files\n",
    "\n",
    "Explanation: \n",
    "        - Files: Get the key (As a string) of each file in a list\n",
    "        - df: Create a dataframe that holds all the values of the days\n",
    "              of the tables we want to extract\n",
    "        \n",
    "        \n",
    "Parameters: bucket, obj, S3 source bucket where we are extracting data\n",
    "            date_list, list, dates of the dataframes we are trying to put together \n",
    "\n",
    "Returns: df, dataframe, combined dataframe with the data of all the dates\n",
    "            \n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def extract(bucket, date_list):\n",
    "    files = [key for date in date_list for key in list_files_in_prefix(bucket, date)]\n",
    "    df = pd.concat([read_csv_to_df(bucket, obj) for obj in files], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: transform_report1(df, columns, arg_date)\n",
    "\n",
    "Purpose: Transform the dataframe to create the new cols we want for the\n",
    "         report\n",
    "\n",
    "Explanation: \n",
    "        - Drop the columns that we do not want from the dataframe\n",
    "        - Drop all the values that have empty spaces\n",
    "        - Create a colum called opening prices\n",
    "        - Create a colum called closing prices\n",
    "        - Create a colum called previous closing price\n",
    "        - Create a colum called change previous closing percentage\n",
    "        \n",
    "        \n",
    "Parameters: df, dateframe, datraframe that must eb transformed\n",
    "            columns, list of strings, desired columns we want\n",
    "            arg_date, date, date that we are inquiring for report\n",
    "\n",
    "Returns: df, dataframe, combined dataframe with the data of all the dates\n",
    "            \n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def transform_report1(df, columns, arg_date):\n",
    "    df = df.loc[:, columns]\n",
    "    df.dropna(inplace=True)\n",
    "    df['opening_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('first')\n",
    "    df['closing_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('last')\n",
    "    df = df.groupby(['ISIN', 'Date'], as_index=False).agg(opening_price_eur=('opening_price', 'min'), closing_price_eur=('closing_price', 'min'), minimum_price_eur=('MinPrice', 'min'), maximum_price_eur=('MaxPrice', 'max'), daily_traded_volume=('TradedVolume', 'sum'))\n",
    "    df['prev_closing_price'] = df.sort_values(by=['Date']).groupby(['ISIN'])['closing_price_eur'].shift(1)\n",
    "    df['change_prev_closing_%'] = (df['closing_price_eur'] - df['prev_closing_price']) / df['prev_closing_price'] * 100\n",
    "    df.drop(columns=['prev_closing_price'], inplace=True)\n",
    "    df = df.round(decimals=2)\n",
    "    df = df[df.Date >= arg_date]\n",
    "    return df\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: load(bucket, df, trg_key, trg_format, meta_key, extract_date_list)\n",
    "\n",
    "Purpose: Load transformed dataframe into target bucket\n",
    "\n",
    "Explanation: \n",
    "        - Create string for the key in the proper format\n",
    "        - Call function to upload file to upload dataframe into bucket\n",
    "        - Update the metafile\n",
    "        \n",
    "        \n",
    "Parameters: bucket, obj, target bucket where we want to load the dataframe\n",
    "            df, dataframe, transformed dataframe\n",
    "            trg_key, string, name of the report we are currenlty targeting\n",
    "            trg_format, string, string of .parquet (Type of file that receives Amazon)\n",
    "            meta_key, string, name of the metafile\n",
    "            extract_date_list, list, list of dates\n",
    "\n",
    "Returns: True\n",
    "            \n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def load(bucket, df, trg_key, trg_format, meta_key, extract_date_list):\n",
    "    key = trg_key + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + trg_format\n",
    "    write_df_to_s3(bucket, df, key)\n",
    "    update_meta_file(bucket, meta_key, extract_date_list) \n",
    "    return True\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: etl_report(src_bucket, trg_bucket, date_list, columns, arg_date, trg_key, trg_format, meta_key)\n",
    "\n",
    "Purpose: Create the table of the extract values we are looking into.\n",
    "\n",
    "Explanation:\n",
    "        - df: call extract(src_bucket, date_list) method to create \n",
    "              Dateframe of the data of the dates choosen.\n",
    "        - df: call transform_report1(df, columns, arg_date) method\n",
    "              to update the dateframe to the reports we want to see\n",
    "        - extract_date_list: list of the dates missing to update in \n",
    "              the target bucket.\n",
    "        - call load(trg_bucket, df, trg_key, trg_format, meta_key, extract_date_list)\n",
    "            to update the method\n",
    "        \n",
    "Parameters: \n",
    "            src_bucket, Object, Source bucket.\n",
    "            trg_bucket, Object, destination bucket.\n",
    "            date_list, list, list of the dates we are updating.\n",
    "            columns, list, columns of our report\n",
    "            arg_date, date, date we are looking to have the reports\n",
    "            trg_key, string, string of the name of the database we will\n",
    "                             be exploring\n",
    "            trg_format, string, the target format we want\n",
    "            meta_key, string, meta key file in the S3 \n",
    "\n",
    "Returns:\n",
    "            True\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def etl_report(src_bucket, trg_bucket, date_list, columns, arg_date, trg_key, trg_format, meta_key):\n",
    "    df = extract(src_bucket, date_list)\n",
    "    df = transform_report1(df, columns, arg_date)\n",
    "    extract_date_list = [date for date in date_list if date >= arg_date]\n",
    "    load(trg_bucket, df, trg_key, trg_format, meta_key, extract_date_list)\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application Layer - Not Core\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: return_date_list(bucket, arg_date, src_format, meta_key)\n",
    "\n",
    "Purpose: Understand the minimum day and the dates that have not been\n",
    "         explored.\n",
    "\n",
    "Explanation:\n",
    "        - min_date: Get the previous date of the arg_date we are \n",
    "                    looking into\n",
    "        - today: Get today's date\n",
    "        \n",
    "        Try Block:\n",
    "            - df_meta: Transform meta csv file into a dateframe\n",
    "            - dates: Creates a list of all the values we have\n",
    "                     not look into. \n",
    "            - src_dates: Creates a set of the source values of the\n",
    "                        meta_file\n",
    "            - dates_missing: Creates a set of the values missing\n",
    "            \n",
    "            If there are dates missing:\n",
    "            \n",
    "                min_date = minimum value of the set\n",
    "                return_dates = list of all values greater than min date\n",
    "                return_min_date = value of min date to be return, one more \n",
    "                                  day than the value of min_date\n",
    "                \n",
    "            Else if no dates missing:\n",
    "                \n",
    "                return_dates = return an empty list\n",
    "                return_min_date = return a date 2200,1,1\n",
    "        \n",
    "        Exception (Any Errors within the connection of the bucket):\n",
    "            - return_dates = list of all values greater than min date\n",
    "            - return_min_date = value of min date to be return, one more \n",
    "                                  day than the value of min_date\n",
    "        \n",
    "Parameters: \n",
    "            Bucket, Object, Source bucket or destination bucket.\n",
    "            df, dataframe, dataframe to be changed to parquet object.\n",
    "            key, String, Specific csv file that will be converted to csv.\n",
    "\n",
    "Returns:\n",
    "            return_min_date, date, last day from where our data was called\n",
    "            return_dates, list, list of dates that have to be explored\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def return_date_list(bucket, arg_date, src_format, meta_key):\n",
    "    min_date = datetime.strptime(arg_date, src_format).date() - timedelta(days=1)\n",
    "    today = datetime.today().date()\n",
    "    try:\n",
    "        df_meta = read_csv_to_df(bucket, meta_key)\n",
    "        dates = [(min_date + timedelta(days=x)) for x in range(0,(today - min_date).days + 1)]\n",
    "        print(dates)\n",
    "        src_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\n",
    "        print(src_dates)\n",
    "        dates_missing = set(dates[1:]) - src_dates\n",
    "        print(dates_missing)\n",
    "        if dates_missing:\n",
    "            min_date = min(set(dates[1:]) - src_dates) - timedelta(days=1)\n",
    "            return_dates = [date.strftime(src_format)for date in dates if date >= min_date]\n",
    "            return_min_date = (min_date + timedelta(days=1)).strftime(src_format)\n",
    "        else:\n",
    "            return_dates = []\n",
    "            return_min_date = datetime(2200,1,1).date()\n",
    "    except bucket.session.client('s3').exceptions.NoSuchKey:\n",
    "        return_dates = [(min_date + timedelta(days=x)).strftime(src_format) for x in range(0,(today - min_date).days + 1)]\n",
    "        return_min_date = arg_date\n",
    "    return return_min_date, return_dates\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Function: update_meta_file(bucket, meta_key, extract_date_list)\n",
    "\n",
    "Purpose: Update our current meta_file\n",
    "\n",
    "Explanation:\n",
    "        - Create new data frame with columns: source_date, date\n",
    "        - Update the values of 'source_date' with new values\n",
    "        - Update the values of 'datetime_of_processing' with new values\n",
    "        - Create in memory df_old to have the current df\n",
    "        - Put both the df_old and df_new together\n",
    "        - Update the meta file in the bucket  \n",
    "        \n",
    "Parameters: \n",
    "            Bucket, Object, Source bucket or destination bucket.\n",
    "            meta_key, string, meta key we are reading\n",
    "            extract_date_list, list, list that would update the metafile\n",
    "\n",
    "Returns:\n",
    "            None\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def update_meta_file(bucket, meta_key, extract_date_list):\n",
    "    df_new = pd.Dateframe(columns=['source_date', 'date'])\n",
    "    df_new['source_date'] = extract_date_list\n",
    "    df_new['datetime_of_processing'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    df_old = read_csv_to_df(bucket, meta_key)\n",
    "    df_all = pd.concat(df_old, df_new)\n",
    "    write_df_to_s3_csv(bucket, df_all, meta_key)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Function Entry Point\n",
    "def main():\n",
    "    #Parameters/Configurations\n",
    "    #Later Read config\n",
    "    arg_date = '2021-10-24'\n",
    "    src_format = '%Y-%m-%d'\n",
    "    src_bucket = 'deutsche-boerse-xetra-pds'\n",
    "    trg_bucket = 'xetra-123'\n",
    "    columns = ['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']\n",
    "    trg_key = 'xetra_daily_report_'\n",
    "    trg_format = '.parquet'\n",
    "    meta_key = 'meta_file.csv'\n",
    "    \n",
    "    #Init our connection\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket_src = s3.Bucket(src_bucket)\n",
    "    bucket_trg = s3.Bucket(trg_bucket)\n",
    "    \n",
    "    #Run Application\n",
    "    extract_date, date_list = return_date_list(bucket_trg, arg_date, src_format, meta_key)\n",
    "    etl_report(bucket_src, bucket_trg, date_list, columns, extract_date, trg_key, trg_format, meta_key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2021, 10, 23), datetime.date(2021, 10, 24), datetime.date(2021, 10, 25), datetime.date(2021, 10, 26), datetime.date(2021, 10, 27), datetime.date(2021, 10, 28)]\n",
      "{datetime.date(2021, 10, 12), datetime.date(2021, 10, 13)}\n",
      "{datetime.date(2021, 10, 28), datetime.date(2021, 10, 26), datetime.date(2021, 10, 27), datetime.date(2021, 10, 24), datetime.date(2021, 10, 25)}\n"
     ]
    },
    {
     "ename": "ReadTimeoutError",
     "evalue": "Read timeout on endpoint URL: \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;31m# cStringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m                 \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/botocore/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mURLLib3ReadTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    540\u001b[0m                         \u001b[0;31m# Content-Length are caught.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_remaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# there is yet no clean way to get at it from this context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: AWSHTTPSConnectionPool(host='deutsche-boerse-xetra-pds.s3.eu-central-1.amazonaws.com', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3549101bcb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-bdc4e07eb04d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#Run Application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mextract_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_date_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_trg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0metl_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_trg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cb246e8add5c>\u001b[0m in \u001b[0;36metl_report\u001b[0;34m(src_bucket, trg_bucket, date_list, columns, arg_date, trg_key, trg_format, meta_key)\u001b[0m\n\u001b[1;32m    121\u001b[0m \"\"\"\n\u001b[1;32m    122\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0metl_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_bucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_bucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_bucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_report1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mextract_date_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0marg_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cb246e8add5c>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(bucket, date_list)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_files_in_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread_csv_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cb246e8add5c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_files_in_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread_csv_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e8eafd58b733>\u001b[0m in \u001b[0;36mread_csv_to_df\u001b[0;34m(bucket, key, decoding, separator)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mcsv_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/botocore/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mURLLib3ReadTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# TODO: the url will be None as urllib3 isn't setting it yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amount_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: Read timeout on endpoint URL: \"None\""
     ]
    }
   ],
   "source": [
    "#Run\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_bucket = 'xetra-123'\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_trg = s3.Bucket(trg_bucket)\n",
    "for obj in bucket_trg.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prq_obj = bucket_trg.Object(key='xetra_daily_report_20211023_230203.parquet').get().get('Body').read()\n",
    "data = BytesIO(prq_obj)\n",
    "df_report = pd.read_parquet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
